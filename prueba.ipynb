{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Untitled\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Clean_Reviews</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>i feel so lucky to have found this used phone ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>nice phone nice up grade from my pantach revue...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>very pleased</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>it works good but it goes slow sometimes but i...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>great phone to replace my lost phone the only ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                            Reviews  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...   \n",
       "1       4  nice phone, nice up grade from my pantach revu...   \n",
       "2       5                                       Very pleased   \n",
       "3       4  It works good but it goes slow sometimes but i...   \n",
       "4       4  Great phone to replace my lost phone. The only...   \n",
       "\n",
       "                                       Clean_Reviews  Word_Count  \n",
       "0  i feel so lucky to have found this used phone ...          70  \n",
       "1  nice phone nice up grade from my pantach revue...          40  \n",
       "2                                       very pleased           2  \n",
       "3  it works good but it goes slow sometimes but i...          17  \n",
       "4  great phone to replace my lost phone the only ...          44  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Amazon_Unlocked_Mobile_small_Part_I.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasar a string\n",
    "df['Clean_Reviews'] = df['Clean_Reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi e v e r y o n e i gave a star rating bought this special product here another link here the html tag as well i highly recommend this prduct i love it all it s special characters i am currently investigating the special device and am excited about the features love it furthermore i found the support really great paid about for it inch version screen some special accented text and words like resume cafe or expose'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = pk.load(open(\"clean_text.pkl\",'rb'))\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_func(text):\n",
    "    '''\n",
    "    Counts words within a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Number of words within a string, integer\n",
    "    ''' \n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_stopwords_func(text):\n",
    "    '''\n",
    "    Removes Stop Words (also capitalized) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without Stop Words\n",
    "    ''' \n",
    "    # check in lowercase \n",
    "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totenizar palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi my name is Michael. I am an enthusiastic Data Scientist. Currently I am working on a post about NLP, more specifically about the Pre-Processing Steps.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_tokenization = \\\n",
    "\"Hi my name is Michael. \\\n",
    "I am an enthusiastic Data Scientist. \\\n",
    "Currently I am working on a post about NLP, more specifically about the Pre-Processing Steps.\"\n",
    "\n",
    "text_for_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Michael', '.', 'I', 'am', 'an', 'enthusiastic', 'Data', 'Scientist', '.', 'Currently', 'I', 'am', 'working', 'on', 'a', 'post', 'about', 'NLP', ',', 'more', 'specifically', 'about', 'the', 'Pre-Processing', 'Steps', '.']\n"
     ]
    }
   ],
   "source": [
    "# teoten word\n",
    "words = word_tokenize(text_for_tokenization)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens found: 30\n"
     ]
    }
   ],
   "source": [
    "# count tokens\n",
    "print('Number of tokens found: ' + str(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi my name is Michael.', 'I am an enthusiastic Data Scientist.', 'Currently I am working on a post about NLP, more specifically about the Pre-Processing Steps.']\n"
     ]
    }
   ],
   "source": [
    "# count sentences\n",
    "sentences = sent_tokenize(text_for_tokenization)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences found: ' + str(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my name is Michael.\n",
      "I am an enthusiastic Data Scientist.\n",
      "Currently I am working on a post about NLP, more specifically about the Pre-Processing Steps.\n"
     ]
    }
   ],
   "source": [
    "## print each sentence in a new line\n",
    "\n",
    "for each_sentence in sentences:\n",
    "    n_words=word_tokenize(each_sentence)\n",
    "    print(each_sentence)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# count the number of the words of each sentence \n",
    "for each_sentence in sentences:\n",
    "    n_words=word_tokenize(each_sentence)\n",
    "    print(len(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', 'i', 'gave', 'a', 'star', 'rating', 'bought', 'this', 'special', 'product', 'here', 'another', 'link', 'here', 'the', 'html', 'tag', 'as', 'well', 'i', 'highly', 'recommend', 'this', 'prduct', 'i', 'love', 'it', 'all', 'it', 's', 'special', 'characters', 'i', 'am', 'currently', 'investigating', 'the', 'special', 'device', 'and', 'am', 'excited', 'about', 'the', 'features', 'love', 'it', 'furthermore', 'i', 'found', 'the', 'support', 'really', 'great', 'paid', 'about', 'for', 'it', 'inch', 'version', 'screen', 'some', 'special', 'accented', 'text', 'and', 'words', 'like', 'resume', 'cafe', 'or', 'expose']\n"
     ]
    }
   ],
   "source": [
    "tokens_clean_text = word_tokenize(clean_text)\n",
    "print(tokens_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 80\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences found: ' + str(len(tokens_clean_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi e v e r y o n e i gave a star rating bought this special product here another link here the html tag as well i highly recommend this prduct i love it all it s special characters i am currently investigating the special device and am excited about the features love it furthermore i found the support really great paid about for it inch version screen some special accented text and words like resume cafe or expose']\n"
     ]
    }
   ],
   "source": [
    "sentences_clean_text = sent_tokenize(clean_text)\n",
    "print(sentences_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences found: ' + str(len(sentences_clean_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords\n",
    "\n",
    "print('Number of sentences found: ' + str(len(sentences_clean_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi e v e r n e gave star rating bought special product another link html tag well highly recommend prduct love special characters currently investigating special device excited features love furthermore found support really great paid inch version screen special accented text words like resume cafe expose'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_wo_stop_words = remove_english_stopwords_func(tokens_clean_text)\n",
    "clean_text_wo_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 48\n"
     ]
    }
   ],
   "source": [
    "print('Number of words: ' + str(word_count_func(clean_text_wo_stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "These Stop Words were found in our example string:\n",
      "\n",
      "['y', 'o', 'i', 'a', 'this', 'here', 'here', 'the', 'as', 'i', 'this', 'i', 'it', 'all', 'it', 's', 'i', 'am', 'the', 'and', 'am', 'about', 'the', 'it', 'i', 'the', 'about', 'for', 'it', 'some', 'and', 'or']\n",
      "\n",
      "Number of Stop Words found: 32\n"
     ]
    }
   ],
   "source": [
    "## show tokens that are stop words\n",
    "\n",
    "stop_words_within_tokens_clean_text = [w for w in tokens_clean_text if w in stopwords.words(\"english\")]\n",
    "\n",
    "print()\n",
    "print('These Stop Words were found in our example string:')\n",
    "print()\n",
    "print(stop_words_within_tokens_clean_text)\n",
    "print()\n",
    "print('Number of Stop Words found: ' + str(len(stop_words_within_tokens_clean_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
